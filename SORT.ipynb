{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### فاز دوم پروژه\n",
        "در قسمت دوم پروژه سعی در ردیابی اشیاء و یا افراد داخل ویدیو را داریم. در این بخش از مدل آماده آموزش دیده شده $YOLO$ استفاده میکنیم برای استخراج جعبه های مرزی اشیاء، سپس با استفاده از الگوریتم $SORT$ سعی در ردیابی این اشیاء داریم\n"
      ],
      "metadata": {
        "id": "QHRRycvpnmiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install filterpy"
      ],
      "metadata": {
        "id": "gbud94xoDM5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD8Riv76hIKj"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Project/ # آدرس فایل های پروژه: این آدرس را به محل فایل های خود تغییر دهید\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights -O config/yolov3.weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models import *\n",
        "from utils import *\n",
        "import imageio\n",
        "import os, sys, time, datetime, random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "from sort import *\n",
        "import cv2\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "TX04gFis5Wa3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Detection\n",
        "در این بخش وزن های آماده مدل $YOLO$ را لود کرده و با تنظیم هایپر پارامترها در نهایت مدل را روی $Cuda$ برده که از سرعت $GPU$ نیز بهره ببریم"
      ],
      "metadata": {
        "id": "sYsuMLHRoL4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_path='config/yolov3.cfg'\n",
        "weights_path='config/yolov3.weights'\n",
        "class_path='config/coco.names'\n",
        "img_size=416\n",
        "conf_thres=0.8\n",
        "nms_thres=0.4\n",
        "\n",
        "# Load model and weights\n",
        "model = Darknet(config_path, img_size=img_size) # تعریف مدل  دیتکشن\n",
        "model.load_weights(weights_path) # لود کردن وزن های شبکه\n",
        "model.cuda() # استفاده از جی پی یو باری سرعت بیتشر مدل\n",
        "model.eval() # مدل از قبل آموزش دیده و تنها باید روی حالت ارزیابی باشد\n",
        "classes = utils.load_classes(class_path)\n",
        "Tensor = torch.cuda.FloatTensor"
      ],
      "metadata": {
        "id": "5ydAWqBN5Tds"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "تابع زیر همانطور که از اسم آن پیداست، با گرفتن یک تصویر روی آن پیش پردازش های اولیه را انجام داده و سپس آن را به مدل $Detection$ داده و خروجی های آن که مختصات جعبه های مرزی است را در خروجی میدهد"
      ],
      "metadata": {
        "id": "df3-5Yszo3Lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_image(img):\n",
        "    # پیش پردازش های اولیه روی تصویر\n",
        "    ratio = min(img_size/img.size[0], img_size/img.size[1])\n",
        "    imw = round(img.size[0] * ratio)\n",
        "    imh = round(img.size[1] * ratio)\n",
        "    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
        "         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
        "                        (128,128,128)),\n",
        "         transforms.ToTensor(),\n",
        "         ])\n",
        "    \n",
        "    # تبدیل تصویر به تنسور برای ورودی شبکه\n",
        "    image_tensor = img_transforms(img).float()\n",
        "    image_tensor = image_tensor.unsqueeze_(0)\n",
        "    input_img = Variable(image_tensor.type(Tensor))\n",
        "   \n",
        "   # استفاده از مدل در حالت ارزیابی و دادن تصویر به مدل آموزش دیده شده\n",
        "    with torch.no_grad():\n",
        "        detections = model(input_img)\n",
        "        detections = utils.non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
        "\n",
        "    return detections[0] # خروجی جعبه های مرزی داخل تصویر خواهد بود"
      ],
      "metadata": {
        "id": "1SW2lvTb5hkz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertMillis(millseconds):\n",
        "    seconds, millseconds = divmod(millseconds, 1000)\n",
        "    minutes, seconds = divmod(seconds, 60)\n",
        "    hours, minutes = divmod(minutes, 60)\n",
        "    day, hours = divmod(hours, 24)\n",
        "    seconds = int(seconds + millseconds/10000)\n",
        "    return f\"{int(hours)}:{int(minutes)}:{int(seconds)}\""
      ],
      "metadata": {
        "id": "h5XjEsyvDert"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tracking\n",
        "در کد زیر یک ویدیو در غالب گیف در ورودی داده میشود. سپس بر روی هر یک از فریم های ویدیو پردازش انجام میشود. هر فریم به عنوان یک تصویر جدا در نظر گرفته شده و ابتدا جعبه های مرزی توسط تابع نوشته شده در قسمت قبل استخراج میشود. سپس با استفاده از مدل ترکینگ $SORT$ ترکینگ در راستای فریم های مختلف انجام میشود. در نهایت نیز فریم های پردازش شده ذخیره میشود و در نهایت یک خروجی گیف دیگر خواهیم داشت که در آن اشیاء ئ افراد ترک میشوند"
      ],
      "metadata": {
        "id": "ovZV6WPXpF_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "videopath = 'video/traffic_1.gif' # آدرس ویدیو وروید را اینجا وارد کنید\n",
        "\n",
        "%pylab inline \n",
        "\n",
        "cmap = plt.get_cmap('tab20b')\n",
        "colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
        "\n",
        "cap = cv2.VideoCapture(videopath) # خواندن ویدو\n",
        "\n",
        "mot_tracker = Sort() # تعریف مدل ترکینگ\n",
        "\n",
        "Frames = [] # برای ذخیره ویدیو نهایی\n",
        "num_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # تعداد فریم های ویدیو\n",
        "\n",
        "for ii in range(num_frame):\n",
        "    timestamp = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "    time_report = convertMillis(timestamp)\n",
        "    ret, frame = cap.read() # خواندن یک فریم از ویدیو\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pilimg = Image.fromarray(frame)\n",
        "    detections = detect_image(pilimg) # دادن فریم خوانده شده به شبکه دیتکشن\n",
        "\n",
        "    img = np.array(pilimg)\n",
        "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
        "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
        "    unpad_h = img_size - pad_y\n",
        "    unpad_w = img_size - pad_x\n",
        "\n",
        "    if detections is not None:\n",
        "        tracked_objects = mot_tracker.update(detections.cpu()) # دادن خروجی دیتکشن به شبکه ترکینگ\n",
        "\n",
        "        unique_labels = detections[:, -1].cpu().unique()\n",
        "        n_cls_preds = len(unique_labels)\n",
        "\n",
        "        ########## نمایش جعبه های مرزی اشیاء و شناسه آن ها بر روی هر فریم ###########\n",
        "        for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
        "            box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
        "            box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
        "            y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
        "            x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
        "\n",
        "            color = colors[int(obj_id) % len(colors)]\n",
        "            color = [i * 255 for i in color]\n",
        "            cls = classes[int(cls_pred)]\n",
        "            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 1) # جعبه مرزی اطراف شیء\n",
        "            cv2.rectangle(frame, (x1, y1-20), (x1+len(cls)*15, y1), color, -1) # جعبه مرزی بالای هر شی ء برای نمایش نوع کلاس  و شناسه شیء\n",
        "            cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,255,255), 1) # متن مربوط به اسم کلاس و شناسه\n",
        "\n",
        "    Frames.append(frame) # ذخیره فریم های پردازش شده\n",
        "\n",
        "###### ذخیره ویدیو نهایی ترک شده ######################\n",
        "with imageio.get_writer(\"out.gif\", mode=\"I\") as writer:  \n",
        "    for idx, frame in enumerate(Frames):\n",
        "        writer.append_data(frame)"
      ],
      "metadata": {
        "id": "MO3uY3GhDjOM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}